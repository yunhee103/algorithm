{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMC3P/jqmiP64Edxyo7T0Ac"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHCU04GihvEX","executionInfo":{"status":"ok","timestamp":1759390932369,"user_tz":-540,"elapsed":9512,"user":{"displayName":"yunhee jeong","userId":"13179588524505521824"}},"outputId":"4e9d53af-13f7-4bfe-e2bc-e04d91c05838"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium[classic-control] in /usr/local/lib/python3.12/dist-packages (1.2.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (4.15.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (0.0.4)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[classic-control]) (2.6.1)\n"]}],"source":["# cartpole 버티기\n","# Q-learning : (off-policy TD control) 방식으로 카트폴 환경을 학습하기\n","# MDP(Markov Decision Process) 기반의 강화학습 알고리즘을 사용\n","# MDP의 5가지 환경 구성요소\n","# S (State ): 에이전트가 놓일 수 있는 모든 가능한 상황\n","# A (Action): 에이전트가 각 상태에서 할 수 있는 모든 가능한 동작\n","# R (Reward)): 에이전트가 상태 s에서 행동 a를 취해 새로운 상태 $s'$에 도달했을 때 받는 보상\n","# P (Transition Probability): 에이전트가 상태 s에서 행동 a를 취했을 때, 다음 상태 $s'$로 이동할 확률\n","# π(Policy)): 어떤 상태에서 어떤 행동을 할지를 결정\n","\n","!pip install gymnasium[classic-control]\n","\n","\n"]},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","from matplotlib.animation import FuncAnimation\n","from IPython.display import HTML\n"],"metadata":{"id":"pgPCGtQBpwDd","executionInfo":{"status":"ok","timestamp":1759390932559,"user_tz":-540,"elapsed":181,"user":{"displayName":"yunhee jeong","userId":"13179588524505521824"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from logging import raiseExceptions\n","# 환경 설정\n","env = gym.make('CartPole-v1') # 카트에 막대 (pole)를 수직으로 세운 채 좌우로 움직여 균형을 유지하는 환경을 제공\n","print(env.observation_space) # 관측 공간\n","# 카트 위치 , 카트 속도 , 막대 기울기, 막대 각 속도\n","obs_space_low = np.array([-2.4, -3.0, -0.5, -2.0])\n","obs_space_high = np.array([2.4, 3.0, 0.5, 2.0])\n","\n","# 상태 공간 이산화 수준 설정. Q-table은 연속적인 상태를 다를 수 없으므로 구간으로 나눠줘야함\n","state_bins = [6, 12, 6, 12] # 카트폴의 4가지 관측값에 대해 4가지 이산화 수준을 순서대로 적용해야 하므로, 순서가 보장되는 리스트나 튜플을 사용\n","\n","q_table = np.zeros(state_bins + [env.action_space.n])\n","print(q_table.shape)\n","\n","# 상태 이산화 처리 함수\n","def discretize_state(state):\n","    ratios = (state - obs_space_low) / (obs_space_high - obs_space_low)\n","    # ex) (0-(-2.4))/(2.4--(-2.4)) =\n","    print('ratios :' , ratios)\n","    discrete = (ratios * state_bins).astype(int) # 구간이 선택됨\n","    print('discrete :' , discrete)\n","\n","    return tuple(np.clip(discrete, 0, np.array(state_bins) -1 )) # 원본 배열, 허용 최소값, 허용 최대값\n","\n","# discretize_state 결과실험\n","\"\"\"\n","ex_state = np.array([1.0, 0.5, 0.1, -1.0])\n","dis_index = discretize_state(ex_state)\n","print('Q-table 인덱스 :', dis_index)\n","\"\"\"\n","# Q-learning의 하이퍼파라메터 설정\n","alpha = 0.1 # Q-테이블 값을 얼마나 빠르게 업데이트할지 결정. 값이 클수록 최근 학습 결과에 더 큰 비중\n","gammer = 0.99 # 미래의 보상을 얼마나 중요하게 생각할지 결정\n","epsilon = 1.0 #초기에 1.0으로 설정하여 에이전트가 무작위로 행동하며 탐험을 많이 하도록\n","epsilon_decay =0.99   # 에피소드가 진행될수록 엡실론 값을 줄여 탐험을 줄이고 학습된 행동을 활용\n","esilon_min =0.05      # 엡실론이 너무 작아지지 않도록 최솟값을 설정\n","episodes = 1000\n","reward_list=[]  # 각 에피소드에서 받은 총 보상\n","trajector = []  # 궤적 위치 저장 - 애니메이션 시각화용\n","best_reward = 0 # 최고의 총 보상 저장"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAUe5h5-qQof","executionInfo":{"status":"ok","timestamp":1759390932755,"user_tz":-540,"elapsed":180,"user":{"displayName":"yunhee jeong","userId":"13179588524505521824"}},"outputId":"6588666a-921f-4184-bc3d-ed04104854a4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n","(6, 12, 6, 12, 2)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"oBAI-8SbtxQu","executionInfo":{"status":"ok","timestamp":1759390932766,"user_tz":-540,"elapsed":9,"user":{"displayName":"yunhee jeong","userId":"13179588524505521824"}}},"execution_count":3,"outputs":[]}]}