{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyON0Q1hnbRS6FmZUQGbwswL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_HE_JXnA466","executionInfo":{"status":"ok","timestamp":1759384403575,"user_tz":-540,"elapsed":24,"user":{"displayName":"yunhee jeong","userId":"13179588524505521824"}},"outputId":"df25dce3-0e0a-48fb-fd8e-635c6ed4c581"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0.]\n"," [0. 0.]\n"," [0. 0.]\n"," [0. 0.]\n"," [0. 0.]]\n","학습된 Q-table : \n","state 0 : 왼쪽 = 6.26, 오른쪽 = 7.26\n","state 1 : 왼쪽 = 6.20, 오른쪽 = 8.09\n","state 2 : 왼쪽 = 6.66, 오른쪽 = 9.00\n","state 3 : 왼쪽 = 7.88, 오른쪽 = 10.00\n","state 4 : 왼쪽 = 0.00, 오른쪽 = 0.00\n"]}],"source":["# 기초적인 Q-learning 연습\n","# 완전한 환경모델 없이 model-free 방식으로 벨만 방적식 기반 (Q값 갱신)의 근사학습 사용\n","# 1차원 선에서 좌/우 이동하며 보상받기\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","\n","state_space = [0,1,2,3,4] # 4 coin\n","action_space = [-1,1]\n","# Q 테이블 초기화 (상태 5 * 행동 2)\n","Q = np.zeros([len(state_space), len(action_space)])\n","print(Q)\n","\n","# 하이퍼 파라메터 줌\n","alpha = 0.1   # 학습률\n","gamma = 0.9   # Υ 할인율(Discounter factor)\n","epsilon = 1.0 # ε-greedy 초기 탐험률 , 100%탐험\n","epsilon_min = 0.1\n","epsilon_decay = 0.99\n","episodes = 100\n","\n","def get_reward(state): # 보상 함수\n","  return 10 if state == 4 else 0\n","\n","# 학습루프 : 각 episode 마다 Q-table을 갱신하면서 목표상태(state=4)에 도달하기 위한 최적의 행동 정책을 학습\n","for episode in range(episodes):\n","  state = 0\n","  for step in range(20): # 하나의 에피소드 안에서 최대 20번 이동 시도\n","  # 행동 선택은 ε-greedy 정책\n","    if random.random() < epsilon:\n","      action_index = random.randint(0,1) # 탐험 exploration\n","    else:\n","      action_index = np.argmax(Q[state,:]) # 이용 exploitation\n","    action = action_space[action_index]  # [-1,1] 왼쪽 오른쪽 이동\n","    next_state = state + action\n","\n","    # 유효 범위 밖은 이동 금지\n","    if next_state < 0:\n","        next_state = 0\n","    if next_state > 4:\n","        next_state = 4\n","\n","    reward = get_reward(next_state)\n","\n","    # 벨만 방정식을 이용해서 Q-value를 갱신\n","    old_q = Q[state][action_index] # 현재 추정된 q 값\n","    next_max = np.max(Q[next_state])  # 다음 상태에서 가능한 모든 행동 중 가장 큰 Q값을 선택\n","    Q[state][action_index] = old_q + alpha * (reward + gamma*next_max-old_q)\n","    state = next_state\n","    if reward == 10:\n","        break\n","\n","  epsilon = max(epsilon_min, epsilon*epsilon_decay)\n","# 결과 출력\n","print('학습된 Q-table : ')\n","for s in range(5):\n","  print(f'state {s} : 왼쪽 = {Q[s][0]:.2f}, 오른쪽 = {Q[s][1]:.2f}')\n"]},{"cell_type":"code","source":[],"metadata":{"id":"GtTaNQaaB8Ue"},"execution_count":null,"outputs":[]}]}